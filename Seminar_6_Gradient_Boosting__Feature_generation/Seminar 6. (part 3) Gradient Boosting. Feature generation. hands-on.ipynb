{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video link https://youtu.be/q0VofqW4g20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart TripType prediction\n",
    "\n",
    "https://www.kaggle.com/c/walmart-recruiting-trip-type-classification\n",
    "\n",
    ">For this competition, you are tasked with categorizing shopping trip types based on the items that customers purchased. To give a few hypothetical examples of trip types: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes.\n",
    "\n",
    "\n",
    "## Multi-class classification, goal is to predict `type of the trip`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/kaggle\", line 5, in <module>\r\n",
      "    from kaggle.cli import main\r\n",
      "  File \"/usr/local/lib/python3.7/site-packages/kaggle/__init__.py\", line 23, in <module>\r\n",
      "    api.authenticate()\r\n",
      "  File \"/usr/local/lib/python3.7/site-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\r\n",
      "    self.config_file, self.config_dir))\r\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /Users/vitalijugnivenko/.kaggle. Or use the environment method.\r\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c walmart-recruiting-trip-type-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File walmart-recruiting-trip-type-classification/train.csv does not exist: 'walmart-recruiting-trip-type-classification/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fa722adedad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'walmart-recruiting-trip-type-classification/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'walmart-recruiting-trip-type-classification/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File walmart-recruiting-trip-type-classification/train.csv does not exist: 'walmart-recruiting-trip-type-classification/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('walmart-recruiting-trip-type-classification/train.csv')\n",
    "test = pd.read_csv('walmart-recruiting-trip-type-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns description\n",
    "\n",
    "- `TripType` - a categorical id representing the type of shopping trip the customer made. This is the ground truth that you are predicting. TripType_999 is an \"other\" category.\n",
    "- `VisitNumber` - an id corresponding to a single trip by a single customer\n",
    "- `Weekday` - the weekday of the trip\n",
    "- `Upc`- the UPC number of the product purchased\n",
    "- `ScanCount` - the number of the given item that was purchased. A negative value indicates a product return.\n",
    "- `DepartmentDescription` - a high-level description of the item's department\n",
    "- `FinelineNumber` - a more refined category for each of the products, created by Walmart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.TripType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understand the task.\n",
    "\n",
    "> Every single ml task has its features, and there are no universal solutions, only generally working principles.\n",
    "\n",
    "We need to predict type of the visit: `TripType`. Every row of the data table contains information about a single product not a visit, threfore we need to combine the information about the visit from all the purchases. \n",
    "\n",
    "![](https://downloader.disk.yandex.ru/preview/efd0ee1162a479288a9ef907b7c627e0216b9b3eb9f85500ee4e0a0a66552c4c/5f6c7f77/eeebh5OVftecfJjMjJc1xYaANWzMwwdQQtKH72IlRPTvgOXDhJidoIyHmzaIk9kp8pZpk9fLNjeTpe27JpbpDg==?uid=0&filename=Screenshot+from+2020-09-24+10-12-50.png&disposition=inline&hash=&limit=0&content_type=image%2Fpng&tknv=v2&owner_uid=159868851&size=2048x2048)\n",
    "\n",
    "image link https://yadi.sk/i/ShJNKZ-42X65NA\n",
    "\n",
    "Unfortunately, we do not have the information about customers, so we do not know if some of the Visits were performed by the same person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a simple baseline model\n",
    "\n",
    "- Count number of purchases in a Visit\n",
    "- Create binary column `is_weekend`\n",
    "- Drop all the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weekend(day):\n",
    "    return int(day in ['Saturday', 'Sunday'])\n",
    "\n",
    "df_train = train.copy()\n",
    "\n",
    "# Generate is_weekend\n",
    "df_train['is_weekend'] = df_train.Weekday.apply(is_weekend)\n",
    "\n",
    "# Generate n_products\n",
    "gp_n_products = df_train.groupby('VisitNumber')['ScanCount'].count()\n",
    "df_train['n_products'] = df_train.VisitNumber.map(gp_n_products)\n",
    "\n",
    "# drop duplicated Visit numbers\n",
    "df_train = df_train.drop_duplicates(subset=['VisitNumber']).reset_index(drop=True)\n",
    "\n",
    "# drop all columns except `is_weekend`, `n_products` and `TripType`\n",
    "df_train = df_train.drop(['VisitNumber', 'Weekday', 'Upc', 'ScanCount',\n",
    "                          'DepartmentDescription', 'FinelineNumber'], axis=1)\n",
    "\n",
    "# Encode TripType so unique values are from 0 to (m-1), where m is number of classes\n",
    "encoder = LabelEncoder().fit(df_train['TripType'])\n",
    "df_train['TripType_encoded'] =  encoder.transform(df_train['TripType'])\n",
    "df_train = df_train.drop('TripType', axis=1)\n",
    "\n",
    "# Create separate variables X and y\n",
    "X = df_train.drop('TripType_encoded', axis=1)\n",
    "y = df_train.TripType_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test, use parameter `stratify=y`\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=774, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a RandomForest model with default hyperparameters\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For this tash we are using a log loss $$- y \\log p - (1-y)\\log(1-p)$$\n",
    "in the following multi class form:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N \\log {\\left(\\frac{e^{a_{it_i}}}{\\sum_{j=0}^{M-1} e^{a_{ij}}}\\right)}$$\n",
    "\n",
    "$t \\in \\{0\\ldots M-1\\}$, $M$ is number of classes, $N$ is number of objects. Numerator is $a_{it_{i}}$ = \\[ unnormalized probability of an $i$'th object to be assigned to the right class $t_i$\\], thus:\n",
    "\n",
    "$$p_{it_i} = \\frac{e^{a_{it_i}}}{\\sum_{j=0}^{M-1} e^{a_{ij}}}$$\n",
    "\n",
    "see for example https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y_test and compute multi-class log loss of your prediction\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get an intuition on whether it is a good a bad prediction compute an accuracy of your model\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember this is a 38 class classification, count predicted classes\n",
    "\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts of a TripType on a train set\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with the constant prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a log loss and an accuracy of a constant prediction (predict most frequent TripType)\n",
    "\n",
    "y_dummy = [5]*19135\n",
    "print(accuracy_score(y_test, y_dummy))\n",
    "\n",
    "y_dummy_proba = np.zeros((19135, 38))\n",
    "y_dummy_proba[:, 5] = 1\n",
    "print(log_loss(y_test, y_dummy_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on the baseline\n",
    "1. Even with this simple features and relatively stupid predictions we are better than a constant prediction.\n",
    "2. Classifier mostly predicts frequent classes.\n",
    "3. Some frequent classes are predicted and some are not. This may be due to the fact that predicted\n",
    "classes are better described by the generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deeper look on the features.\n",
    "\n",
    "### Columns description\n",
    "\n",
    "- `TripType` - a categorical id representing the type of shopping trip the customer made. This is the ground truth that you are predicting. TripType_999 is an \"other\" category.\n",
    "- `VisitNumber` - an id corresponding to a single trip by a single customer\n",
    "- `Weekday` - the weekday of the trip\n",
    "- `Upc`- the UPC number of the product purchased\n",
    "- `ScanCount` - the number of the given item that was purchased. A negative value indicates a product return.\n",
    "- `DepartmentDescription` - a high-level description of the item's department\n",
    "- `FinelineNumber` - a more refined category for each of the products, created by Walmart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features, except `ScanCount` are categorical, a negative value of a `ScanCount` indicates a product return.\n",
    "\n",
    "### 3.1 VisitNumber \n",
    "is an indicator of a visit, we need it to aggregate different purchases,\n",
    "but the number itself is not important it is just an index. Let's have a quick look on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique VisitNumber are in the data?\n",
    "\n",
    "train.VisitNumber.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the typical size of a purchase (number of unique products in a Visit)?\n",
    "\n",
    "sns.histplot(data=train.VisitNumber.value_counts(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train.VisitNumber.value_counts().value_counts().sort_index()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "- More than half of all Visit consists of 4 or less purchases\n",
    "- 90% of Visits consist of 17 or less purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many visits are on different weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday','Thursday','Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "train.Weekday.value_counts().reindex(weekdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have more returns on some weekdays?\n",
    "\n",
    "train['IsReturn'] = train['ScanCount'].apply(lambda x: x < 0)\n",
    "train.groupby(['Weekday'])['IsReturn'].sum().sort_values().reindex(weekdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there only `-1` returns?\n",
    "\n",
    "train[train.IsReturn].ScanCount.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 DepartmentDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most popular Departments? (total sum over ScanCount)\n",
    "\n",
    "train.groupby('DepartmentDescription').ScanCount.sum().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does most popular DepartmentDescription differ for different TripTypes?\n",
    "# > Allows to more or less deanonymize `TripType`.\n",
    "\n",
    "gp = train.groupby('TripType')['DepartmentDescription'].value_counts().reset_index(name='Count')\n",
    "gp2 = gp.groupby(['TripType'])[['Count','DepartmentDescription']]\\\n",
    "                                                           .apply(pd.DataFrame.nlargest, n=3, columns=['Count'])\\\n",
    "                                                           .reset_index()\\\n",
    "                                                           .drop('level_1', axis=1)\n",
    "\n",
    "gp2[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From our baseline, recall most popular TripTypes, let's deanonymize them.\n",
    "# Select subset of the previous table with TripType in [40, 39, 37, 38, 25]\n",
    "\n",
    "gp2[gp2.TripType.isin([40, 39, 37, 38, 25])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the DepartmentDescription with most returns?\n",
    "\n",
    "train.groupby('DepartmentDescription')['IsReturn'].sum().sort_values(ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 FinelineNumber\n",
    "\n",
    "> according to the data description `FinelineNumber` is just a more detailed `DepartmentDescription`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a crosstab between DepartmentDescription and FinelineNumber,\n",
    "# for most popular DepartmentDescription (total ScanCount > 20_000)\n",
    "# and most popular FinelineNumber (total ScanCount > 2000)\n",
    "\n",
    "\n",
    "popular_dd = train.groupby('DepartmentDescription').ScanCount.sum().sort_values(ascending=False)[:10].index\n",
    "sub = train[train.DepartmentDescription.isin(popular_dd)]\n",
    "sub = sub[sub.FinelineNumber.isin(sub.FinelineNumber.value_counts().iloc[:15].index)]\n",
    "\n",
    "tab = pd.crosstab(sub.FinelineNumber, sub.DepartmentDescription)\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Upc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Upc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions on EDA\n",
    "\n",
    " - Visits are mostly consist of small number of products\n",
    " - Large visits are on weekends\n",
    " - `TripType` depends on `DepartmentDescription` and `FinelineNumber`\n",
    " - `Upc` is something like a bar code, could be usefull but contains almost 100k unique values\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('walmart-recruiting-trip-type-classification/train.csv')\n",
    "test = pd.read_csv('walmart-recruiting-trip-type-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_nth(x, n=0):\n",
    "    try:\n",
    "        return x[n]\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def return_nth_val(x, n=0):\n",
    "    try:\n",
    "        return x[n]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def is_weekend(x):\n",
    "    return int(x in ['Sunday', 'Saturday'])\n",
    "\n",
    "def has_return(x):\n",
    "    return int(any(_x < 0 for _x in x))\n",
    "\n",
    "def sum_return(x):\n",
    "    return np.sum([_x for _x in x if _x < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Most frequent DepartmentDescription in the purchase, second most frequent, third, fourth.\n",
    "\n",
    "    f = lambda grp: grp.value_counts().nlargest(10)\n",
    "    x = df.groupby('VisitNumber')['DepartmentDescription'].apply(f).reset_index()\n",
    "    gp = x.groupby('VisitNumber')['level_1'].unique()\n",
    "\n",
    "    df['PopularCategory'] = df.VisitNumber.map(gp)\n",
    "    for i in range(4):\n",
    "        df[f'Category_{i}'] = df['PopularCategory'].apply(return_nth, args=[i])\n",
    "\n",
    "    # 2. Same for FinelineNumber, but 6\n",
    "\n",
    "    x = df.groupby('VisitNumber')['FinelineNumber'].apply(f).reset_index()\n",
    "    gp = x.groupby('VisitNumber')['level_1'].unique()\n",
    "\n",
    "    df['PopularFineline'] = df.VisitNumber.map(gp)\n",
    "    for i in range(6):\n",
    "        df[f'Fineline_{i}'] = df['PopularFineline'].apply(return_nth, args=[i]).astype(object)\n",
    "\n",
    "    # 3. Count number of unique DepartmentDescription in the purchase\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['DepartmentDescription'].nunique()\n",
    "    df['#Unique_Department'] = df.VisitNumber.map(gp)\n",
    "\n",
    "    # 4. Count number of unique FinelineNumber in the purchase.\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['FinelineNumber'].nunique()\n",
    "    df['#Unique_Fineline'] = df.VisitNumber.map(gp)\n",
    "\n",
    "    # 5. Count ScanCount (number of unique Upc in the purchase)\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].count()\n",
    "    df['#UniqueScanCount'] = df['VisitNumber'].map(gp)   \n",
    "\n",
    "    # 6. Sum ScanCount\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].sum()\n",
    "    df['TotalScanCount'] = df['VisitNumber'].map(gp)\n",
    "\n",
    "    # 7. Is weekend\n",
    "\n",
    "    df['is_Weekend'] = df['Weekday'].apply(is_weekend).astype(object)\n",
    "\n",
    "    # 8. Sum returns\n",
    "\n",
    "    gp = df.groupby('VisitNumber')['ScanCount'].apply(sum_return)\n",
    "    df['total_return'] = df.VisitNumber.map(gp)\n",
    "    df['total_return'].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop old and intermediate features\n",
    "\n",
    "    df = df.drop(['Upc', 'ScanCount', 'DepartmentDescription',\n",
    "                  'FinelineNumber', 'PopularFineline', 'PopularCategory'], axis=1)\n",
    "\n",
    "    # Drop duplicated rows (with the same VisitNumber)\n",
    "\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = generate_features(train)\n",
    "# train.to_csv('gb_generated_features_train.csv', index=False)\n",
    "\n",
    "train = pd.read_csv('gb_generated_features_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = generate_features(test)\n",
    "# test.to_csv('gb_generated_features_test.csv', index=False)\n",
    "\n",
    "test = pd.read_csv('gb_generated_features_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a model\n",
    "\n",
    "We will use CatBoost implementation of a GradientBoosting algorithm https://catboost.ai, since our data have many\n",
    "categorical features and catboost implements automatical categorical features handling.\n",
    "\n",
    "https://catboost.ai/docs/concepts/python-usages-examples.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names, thier index and datatype\n",
    "\n",
    "for i, col in enumerate(train.columns):\n",
    "    print(i-2, col, train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost requiers all categorical features to be `str` or `int`, and all missing values to be `str`.\n",
    "\n",
    "for col in train.columns:\n",
    "    if col.startswith('Category'):\n",
    "        train[col]=train[col].apply(str)\n",
    "    if col.startswith('Fineline'):\n",
    "        train[col]=train[col].apply(str)\n",
    "        \n",
    "train.fillna('NaN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "\n",
    "x_train, x_eval = train_test_split(train, test_size=0.1, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could provide catboost indexes of all categorical features\n",
    "\n",
    "cat_features = [*list(range(10)), 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pool objects (catboost internal data structure)\n",
    "\n",
    "data_train = Pool(x_train.drop(['TripType', 'VisitNumber'], axis=1), \n",
    "                  label=x_train.TripType,\n",
    "                  cat_features=cat_features)\n",
    "\n",
    "data_eval = Pool(x_eval.drop(['TripType', 'VisitNumber'], axis=1), \n",
    "                 label=x_eval.TripType,\n",
    "                 cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model\n",
    "\n",
    "\n",
    "> If you have a GPU it will drastically improve training speed, use parameter `device='GPU'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a CatBoostClassifier with the following set of hyperparameters\n",
    "\n",
    "ctb_params = {\n",
    "    'depth': 4,\n",
    "    'learning_rate': .33,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'loss_function': 'MultiClass',\n",
    "    'verbose': 1,\n",
    "    'thread_count': 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With this particular features and hyperparameters it takes about 1.5 hours on a CPU\n",
    "\n",
    "# model = CatBoostClassifier(**ctb_params)\n",
    "# model.fit(X = data_train, silent=False, eval_set=data_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://downloader.disk.yandex.ru/preview/47a45d0202eecc7b3d8fb35050278a2714c430c51af256ed919379e6a54c2c5d/5f7074cc/bc4nyN3t5JFfbprrYUVJamEuORedzsk-VmIkZ9PIafohTWWT9tiuZj6n0fo_c9npMfoJXu57fO-EZQkbM2vKyQ==?uid=0&filename=Screenshot+from+2020-09-27+10-16-51.png&disposition=inline&hash=&limit=0&content_type=image%2Fpng&tknv=v2&owner_uid=159868851&size=2048x2048)\n",
    "\n",
    "pic. link https://yadi.sk/i/IFRhOFu_y_PAKw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results into file\n",
    "\n",
    "- save model \n",
    "- save feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_model('catboost_depth=4_lr=.33_l2=.3')\n",
    "# feature_importance = dict(zip(model.feature_names_, model.get_feature_importance()))\n",
    "# with open('feature_importance_cb.json', 'w') as f:\n",
    "#     json.dump(feature_importance, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model = model.load_model('catboost_depth=4_lr=.33_l2=.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log loss on the local train and test data\n",
    "\n",
    "log_loss(x_train.TripType, model.predict_proba(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(x_eval.TripType, model.predict_proba(data_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy score on the local train and test data\n",
    "\n",
    "accuracy_score(x_eval.TripType, model.predict(data_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(x_train.TripType, model.predict(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the feature importance\n",
    "\n",
    "https://catboost.ai/docs/concepts/fstr.html\n",
    "\n",
    "some other possible approaches:\n",
    "- https://github.com/slundberg/shap\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved feature importance\n",
    "\n",
    "with open('feature_importance_cb.json', 'r') as f:\n",
    "    feature_importance = json.load(f)\n",
    "    \n",
    "feature_importance = sorted(feature_importance.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barchart of feature importances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,10))\n",
    "\n",
    "colnames = [fi[0] for fi in feature_importance]\n",
    "importance = [fi[1] for fi in feature_importance]\n",
    "\n",
    "ax.barh(range(len(colnames)), importance)\n",
    "ax.set_yticks(range(len(colnames)))\n",
    "ax.set_yticklabels(colnames);\n",
    "ax.grid();\n",
    "ax.set_title('Feature importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "1. Analyze where does your model make most mistakes, e.g. which classes are not predicted, which classes are confused by a model (build a confusion matrix).\n",
    "2. Based on (1) generate more features, e.g. \"Percentage of products from the top 1 `DepartmentCategory`\".\n",
    "3. GridSearch over boosting parameters. Fit best model on a whole train data. \n",
    "https://catboost.ai/docs/concepts/python-reference_catboost_grid_search.html\n",
    "4. Think about cross-validation, e.g. time-based split? upsample rare classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
